{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10677574,"sourceType":"datasetVersion","datasetId":6614244}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport datetime\nfrom pathlib import Path\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom tqdm import tqdm\n\nimport nltk\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:22.927497Z","iopub.execute_input":"2025-02-14T13:19:22.927730Z","iopub.status.idle":"2025-02-14T13:19:26.310728Z","shell.execute_reply.started":"2025-02-14T13:19:22.927708Z","shell.execute_reply":"2025-02-14T13:19:26.309816Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from functools import cache\nfrom typing import List\nimport nltk\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef kl_divergence(p, q):\n    \"\"\"\n    Compute the KL divergence between two distributions\n    \"\"\"\n    return torch.nan_to_num(p * (p / q).log(), nan=0.0).sum(-1)\n\n\ndef jensen_shannon_divergence(p, q):\n    \"\"\"\n    Compute the Jensen-Shannon divergence between two distributions\n    \"\"\"\n    m = 0.5 * (p + q)\n    return 0.5 * (kl_divergence(p, m) + kl_divergence(q, m))\n\n\nclass RSAReranking:\n    \"\"\"\n    Rerank a list of candidates according to the RSA model.\n    \"\"\"\n\n    def __init__(\n            self,\n            model,\n            tokenizer,\n            candidates: List[str],\n            source_texts: List[str],\n            batch_size: int = 32,\n            rationality: int = 1,\n            device=\"cpu\",\n    ):\n        \"\"\"\n        :param model: hf model used to compute the likelihoods (supposed to be a seq2seq model), is S0 in the RSA model\n        :param tokenizer:\n        :param candidates: list of candidates summaries\n        :param source_texts: list of source texts\n        :param batch_size: batch size used to compute the likelihoods (can be high since we don't need gradients and\n        it's a single forward pass)\n        :param rationality: rationality parameter of the RSA model\n        :param device: device used to compute the likelihoods\n        \"\"\"\n        self.model = model\n        self.device = device\n        self.tokenizer = tokenizer\n\n        self.candidates = candidates\n        self.source_texts = source_texts\n\n        self.batch_size = batch_size\n        self.rationality = rationality\n        print(\"hello this is the test version!\")\n\n    def compute_conditionned_likelihood(\n            self, x: List[str], y: List[str], mean: bool = True\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute the likelihood of y given x\n\n        :param x: list of source texts len(x) = batch_size\n        :param y: list of candidates summaries len(y) = batch_size\n        :param mean: average the likelihoods over the tokens of y or take the sum\n        :return: tensor of shape (batch_size) containing the likelihoods of y given x\n        \"\"\"\n\n        assert len(x) == len(y), \"x and y must have the same length\"\n\n        loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n        batch_size = len(x)\n\n        x = self.tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n        y = self.tokenizer(y, return_tensors=\"pt\", padding=True, truncation=True)\n        # Concatenate the two inputs\n        # Compute the likelihood of y given x\n\n        x_ids = x.input_ids.to(self.device)\n        y_ids = y.input_ids.to(self.device)\n\n        logits = self.model(\n            input_ids=x_ids,\n            decoder_input_ids=y_ids,\n            attention_mask=x.attention_mask.to(self.device),\n            decoder_attention_mask=y.attention_mask.to(self.device),\n        ).logits\n        # Compute the likelihood of y given x\n\n        shifted_logits = logits[..., :-1, :].contiguous()\n        shifted_ids = y_ids[..., 1:].contiguous()\n\n        likelihood = -loss_fn(\n            shifted_logits.view(-1, shifted_logits.size(-1)), shifted_ids.view(-1)\n        ) #comment: maybe  quality_score can be implemented here\n\n        likelihood = likelihood.view(batch_size, -1).sum(-1)\n        if mean:\n            likelihood /= (y_ids != self.tokenizer.pad_token_id).float().sum(-1)\n\n        return likelihood\n\n    def score(self, x: List[str], y: List[str], **kwargs):\n        return self.compute_conditionned_likelihood(x, y, **kwargs)\n\n    def likelihood_matrix(self) -> torch.Tensor:\n        \"\"\"\n        :return: likelihood matrix : (world_size, num_candidates), likelihood[i, j] is the likelihood of\n        candidate j being a summary for source text i.\n        \"\"\"\n        likelihood_matrix = torch.zeros(\n            (len(self.source_texts), len(self.candidates))\n        ).to(self.device)\n\n        pairs = []\n        for i, source_text in enumerate(self.source_texts):\n            for j, candidate in enumerate(self.candidates):\n                pairs.append((i, j, source_text, candidate))\n\n        # split the pairs into batches\n        batches = [\n            pairs[i: i + self.batch_size]\n            for i in range(0, len(pairs), self.batch_size)\n        ]\n\n        for batch in tqdm(batches):\n            # get the source texts and candidates\n            source_texts = [pair[2] for pair in batch]\n            candidates = [pair[3] for pair in batch]\n\n            # compute the likelihoods\n            with torch.no_grad():\n                likelihoods = self.score(\n                    source_texts, candidates, mean=True\n                )\n\n            # fill the matrix\n            for k, (i, j, _, _) in enumerate(batch):\n                likelihood_matrix[i, j] = likelihoods[k].detach()\n\n        return likelihood_matrix\n\n    @cache\n    def S(self, t):\n        if t == 0:\n            return self.initial_speaker_probas\n        else:\n            listener = self.L(t - 1)\n            prod = listener * self.rationality # + self.initial_speaker_probas.sum(0, keepdim=True)\n            return torch.log_softmax(prod, dim=-1)\n\n    @cache\n    def L(self, t):\n        speaker = self.S(t)\n        return torch.log_softmax(speaker, dim=-2)\n\n    def mk_listener_dataframe(self, t): ## add here computation of uniqueness scores (comment: mattia)\n        self.initial_speaker_probas = self.likelihood_matrix()\n\n        initial_listener_probas = self.L(0)\n\n        # compute consensus\n        uniform_distribution_over_source_texts = torch.ones_like(\n            initial_listener_probas\n        ) / len(self.source_texts)\n\n        initital_consensuality_score = (\n                torch.exp(initial_listener_probas)\n                * (\n                        initial_listener_probas - torch.log(uniform_distribution_over_source_texts)\n                )\n        ).sum(0).cpu().numpy()\n\n        initital_consensuality_score = pd.Series(initital_consensuality_score, index=self.candidates)\n\n        initial_listener_probas = initial_listener_probas.cpu().numpy()\n\n        initial_listener_probas = pd.DataFrame(initial_listener_probas)\n        initial_listener_probas.index = self.source_texts\n        initial_listener_probas.columns = self.candidates\n\n        initial_speaker_probas = self.S(0).cpu().numpy()\n        initial_speaker_probas = pd.DataFrame(initial_speaker_probas)\n        initial_speaker_probas.index = self.source_texts\n        initial_speaker_probas.columns = self.candidates\n        listener_probas = self.L(t)\n        listener_df = pd.DataFrame(listener_probas.cpu().numpy())\n        print(f\"I have {listener_probas.shape} listener probabilities\")\n\n        consensuality_scores = (\n                torch.exp(listener_probas)\n                * (listener_probas - torch.log(uniform_distribution_over_source_texts))\n        ).sum(0).cpu().numpy()\n        print(f\"I have {consensuality_scores.shape} consensuality scores\")\n        print(f\"I have {len(self.candidates)} candidates\")\n        consensuality_scores = pd.Series(consensuality_scores, index=self.candidates)\n        \n        # Compute uniqueness score\n        listener_probas_transposed = listener_probas.T \n        uniform_distribution = torch.ones_like(listener_probas_transposed) / len(self.source_texts)\n        uniqueness_scores = kl_divergence(\n            torch.exp(listener_probas_transposed),  # Convert log probabilities to probabilities\n            uniform_distribution\n        ).sum(0).cpu().numpy()##TODO: check code in deepseek. it does .sum(1). should it be .sum(0) on the transposed?\n        uniqueness_scores = pd.Series(uniqueness_scores, index=self.candidates)\n        print(f\"I have {uniqueness_scores.shape} uniqueness scores\")\n        print(f\"I have {len(self.candidates)} candidates\")\n        S = self.S(t).cpu().numpy()\n        speaker_df = pd.DataFrame(S)\n\n        # add the source texts and candidates as index\n\n        listener_df.index = self.source_texts\n        speaker_df.index = self.source_texts\n\n        listener_df.columns = self.candidates\n        speaker_df.columns = self.candidates\n\n        \n\n        return listener_df, speaker_df, initial_listener_probas, initial_speaker_probas, initital_consensuality_score, consensuality_scores, uniqueness_scores\n\n    def rerank(self, t=1):\n        \"\"\"\n        return the best summary (according to rsa) for each text\n        \"\"\"\n        (\n            listener_df,\n            speaker_df,\n            initial_listener_proba,\n            initial_speaker_proba,\n            initital_consensuality_score,\n            consensuality_scores,\n            uniqueness_scores\n        ) = self.mk_listener_dataframe(t=t) #in this function you should compute the uniqueness scores\n        n_unique_sentences = 5\n        best_rsa = speaker_df.idxmax(axis=1).values\n        best_base = initial_listener_proba.idxmax(axis=1).values\n        top_n_unique_list = uniqueness_scores.nlargest(n_unique_sentences).index.tolist()\n        top_n_common_list = uniqueness_scores.nsmallest(n_unique_sentences).index.tolist()\n        best_uniqueness =list(set(top_n_unique_list).union(set(top_n_common_list))) #uniqueness_scores.idxmax().values ##should you remove axis=1\n        #Glimpse unique is combination of top 5 unique and top 5 common.\n\n        return (\n            best_rsa,\n            best_base,\n            best_uniqueness,\n            speaker_df,\n            listener_df,\n            initial_listener_proba,\n            initial_speaker_proba,\n            initital_consensuality_score,\n            consensuality_scores,\n            uniqueness_scores\n        )\n\n\nclass RSARerankingEmbedder(RSAReranking):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def compute_embeddings(self, x: List[str], y: List[str], **kwargs):\n        model_kwargs = kwargs.get(\"model_kwargs\")\n\n        # shape: (batch_size, embedding_dim)\n        x_embeddings = self.model.encode(x, **model_kwargs)\n        y_embeddings = self.model.encode(y, **model_kwargs)\n\n        # dot product between the embeddings : shape (batch_size)\n        dot_products = (x_embeddings * y_embeddings).sum(-1)\n\n        return dot_products\n\n    def score(self, x: List[str], y: List[str], **kwargs):\n        return self.compute_embeddings(x, y, **kwargs)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:26.312237Z","iopub.execute_input":"2025-02-14T13:19:26.312744Z","iopub.status.idle":"2025-02-14T13:19:29.737488Z","shell.execute_reply.started":"2025-02-14T13:19:26.312714Z","shell.execute_reply":"2025-02-14T13:19:29.736780Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"ciao da mattia!","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\n# Percorso dei file caricati\ninput_path = \"/kaggle/input/myfiles/\"\n\n# Lista i file nella directory\nprint(os.listdir(input_path))\nsys.path.append(\"/kaggle/input/myfiles/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:29.738398Z","iopub.execute_input":"2025-02-14T13:19:29.738610Z","iopub.status.idle":"2025-02-14T13:19:29.746996Z","shell.execute_reply.started":"2025-02-14T13:19:29.738592Z","shell.execute_reply":"2025-02-14T13:19:29.746161Z"}},"outputs":[{"name":"stdout","text":"['rsasumm', 'data']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def prepare_dataset(dataset_path) -> Dataset:\n    \n    try:\n        dataset = pd.read_csv(dataset_path)\n    except:\n        raise ValueError(f\"Unknown dataset {dataset_path}\")\n\n    # make a dataset from the dataframe\n    dataset = Dataset.from_pandas(dataset)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:29.747785Z","iopub.execute_input":"2025-02-14T13:19:29.748106Z","iopub.status.idle":"2025-02-14T13:19:29.760818Z","shell.execute_reply.started":"2025-02-14T13:19:29.748077Z","shell.execute_reply":"2025-02-14T13:19:29.760037Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def evaluate_summarizer(dataset: Dataset) -> Dataset:\n    \"\"\"\n    @param dataset: A dataset with the text\n    @return: The same dataset with the summaries added\n    \"\"\"\n    # create a dataset with the text and the summary\n\n    # create a dataloader\n\n    # generate summaries\n    summaries = []\n    print(\"Generating summaries...\")\n\n    # (tqdm library for progress bar) \n    for sample in tqdm(dataset):\n        text = sample[\"text\"] \n        \n        text = text.replace('-----', '\\n')\n        sentences = nltk.sent_tokenize(text)\n        # remove empty sentences\n        sentences = [sentence for sentence in sentences if sentence != \"\"]\n\n        summaries.append(sentences)\n\n    # add summaries to the huggingface dataset\n    dataset = dataset.map(lambda example: {\"summary\": summaries.pop(0)})\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:29.762567Z","iopub.execute_input":"2025-02-14T13:19:29.762797Z","iopub.status.idle":"2025-02-14T13:19:29.778225Z","shell.execute_reply.started":"2025-02-14T13:19:29.762767Z","shell.execute_reply":"2025-02-14T13:19:29.777317Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Percorso della nuova cartella\ndata_dir = \"/kaggle/working/data/candidates\"\n\n# Creare la cartella se non esiste\nos.makedirs(data_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:29.779622Z","iopub.execute_input":"2025-02-14T13:19:29.779855Z","iopub.status.idle":"2025-02-14T13:19:29.795147Z","shell.execute_reply.started":"2025-02-14T13:19:29.779835Z","shell.execute_reply":"2025-02-14T13:19:29.794391Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### 1. Extract candidates","metadata":{}},{"cell_type":"code","source":"# load the dataset\nlimit = 10000\nprint(\"Loading dataset...\")\ndataset_path = \"/kaggle/input/myfiles/data/processed/all_reviews_companies.csv\"\ndataset = prepare_dataset(dataset_path)\n\n# limit the number of samples\nif limit is not None:\n    _lim = min(limit, len(dataset))\n    dataset = dataset.select(range(_lim))\n\n# generate summaries\ndataset = evaluate_summarizer(\n    dataset,\n)\n\ndf_dataset = dataset.to_pandas()\ndf_dataset = df_dataset.explode(\"summary\")\ndf_dataset = df_dataset.reset_index()\n# add an idx with  the id of the summary for each example\ndf_dataset[\"id_candidate\"] = df_dataset.groupby([\"index\"]).cumcount()\n\n# save the dataset\n# add unique date in name\nnow = datetime.datetime.now()\ndate = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\noutput_dir = \"/kaggle/working/data/candidates/\"\noutput_path = (\n    Path(output_dir)\n    / f\"extractive_sentences-_-none-_-{date}.csv\" #f\"extractive_sentences-{date}.csv\"\n    \n)\noutput_path = f\"/kaggle/working/data/candidates/extractive_sentences-_-none-_-{date}.csv\"\n\n# create output dir if it doesn't exist\n#if not output_path.parent.exists():\n#    output_path.parent.mkdir(parents=True, exist_ok=True)\n\ndf_dataset.to_csv(output_path, index=False, encoding=\"utf-8\")\n\n# in case of scripted run, print the output path\nprint(f\"output_path: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:29.795961Z","iopub.execute_input":"2025-02-14T13:19:29.796194Z","iopub.status.idle":"2025-02-14T13:19:29.954944Z","shell.execute_reply.started":"2025-02-14T13:19:29.796174Z","shell.execute_reply":"2025-02-14T13:19:29.954178Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nGenerating summaries...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:00<00:00, 3269.19it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"968d7759ede142f9a255abc6a0d786c9"}},"metadata":{}},{"name":"stdout","text":"output_path: /kaggle/working/data/candidates/extractive_sentences-_-none-_-2025-02-14-13-19-29.csv\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df_dataset.iloc[1][\"summary\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:29.955826Z","iopub.execute_input":"2025-02-14T13:19:29.956073Z","iopub.status.idle":"2025-02-14T13:19:29.961259Z","shell.execute_reply.started":"2025-02-14T13:19:29.956052Z","shell.execute_reply":"2025-02-14T13:19:29.960493Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'However, my recent experience has been frustrating.'"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### 2. Compute RSA","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PegasusTokenizer\nimport argparse\nfrom tqdm import tqdm\n\nfrom pickle import dump\n#from rsasumm.rsa_reranker import RSAReranking\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:29.962025Z","iopub.execute_input":"2025-02-14T13:19:29.962228Z","iopub.status.idle":"2025-02-14T13:19:33.688452Z","shell.execute_reply.started":"2025-02-14T13:19:29.962210Z","shell.execute_reply":"2025-02-14T13:19:33.687298Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:33.689557Z","iopub.execute_input":"2025-02-14T13:19:33.690170Z","iopub.status.idle":"2025-02-14T13:19:33.768537Z","shell.execute_reply.started":"2025-02-14T13:19:33.690137Z","shell.execute_reply":"2025-02-14T13:19:33.767658Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"debug = False\ndef parse_summaries(path: Path) -> pd.DataFrame:\n    \n    try:\n        summaries = pd.read_csv(path)\n    except:\n        raise ValueError(f\"Unknown dataset {path}\")\n\n    # check if the dataframe has the right columns\n    if not all(\n        col in summaries.columns for col in [\"index\", \"id\", \"text\", \"gold\", \"summary\", \"id_candidate\"]\n    ):\n        raise ValueError(\n            \"The dataframe must have columns ['index', 'id', 'text', 'gold', 'summary', 'id_candidate']\"\n        )\n\n    return summaries\n\n\ndef compute_rsa(summaries: pd.DataFrame, model, tokenizer, device):\n    results = []\n    for name, group in tqdm(summaries.groupby([\"id\"])):\n        print(name)\n        if debug:\n            print(\"---candidates---\")\n            print(group.summary.unique().tolist())\n            print(\"---end candidates---\")\n            #print number of candidates\n            print(f\"number of candidates:  {len(group.summary.unique().tolist())}\")\n            #TODO: based on reviews_app.py at line 113, compute uniqueness scores\n            #candidates = group.summary.unique().tolist()\n            #speaker_df = speaker_df.applymap(lambda x: math.exp(x))\n            #for candidate in candidates:\n            #    get sentences of the candidate text_sentences=...\n            #    text_1_summaries = speaker_df.loc[candidate][text_sentences]\n        if not debug:\n            #print(len(group.summary.unique().tolist()),group.summary.unique().tolist())\n            rsa_reranker = RSAReranking(\n                model,\n                tokenizer,\n                device=device,\n                candidates=group.summary.unique().tolist(), #TODO: check what is this.\n                source_texts=group.text.unique().tolist(),\n                #batch_size=32,\n                rationality=3,\n            )\n            (\n                best_rsa,\n                best_base,\n                best_uniqueness,\n                speaker_df,\n                listener_df,\n                initial_listener,\n                language_model_proba_df,\n                initial_consensuality_scores,\n                consensuality_scores,\n                uniqueness_scores,\n            ) = rsa_reranker.rerank(t=2) #maybe you should return here the uniqueness scores\n        \n            gold = group['gold'].tolist()[0]\n            results.append(\n                {\n                    \"id\": name,\n                    \"best_rsa\": best_rsa,  # best speaker score\n                    \"best_base\": best_base,  # naive baseline\n                    \"best_uniqueness\": best_uniqueness,  # naive baseline\n                    \"speaker_df\": speaker_df,  # all speaker results\n                    \"listener_df\": listener_df,  # all listener results (chances of guessing correctly)\n                    \"initial_listener\": initial_listener,\n                    \"language_model_proba_df\": language_model_proba_df,\n                    \"initial_consensuality_scores\": initial_consensuality_scores,\n                    \"consensuality_scores\": consensuality_scores,  # uniqueness scores # TODO: did you write or it was already there? Answer: no, I did not write it. Ask professor how to extract it from the consensuality_scores\n                    \"uniqueness_scores\": uniqueness_scores,  # uniqueness scores\n                    \"gold\": gold,\n                    \"rationality\": 3,  # hyperparameter\n                    \"text_candidates\" : group\n                }\n            )\n    if not debug:\n        return results\n    else:\n        return None\n\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"google/pegasus-xsum\"\nsummaries=output_path\n# load the model and the tokenizer\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nif \"pegasus\" in model_name: \n    print(\"Loading Pegasus Tokenizer\")\n    tokenizer = PegasusTokenizer.from_pretrained(model_name)\nelse:\n    print(\"Loading Auto Tokenizer\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\nprint(\"Model loaded\")\nmodel = model.to(device)\nprint(\"Model to device\")\n# load the summaries\nsummaries = parse_summaries(summaries)\nprint(\"Summaries loaded\")\n# rerank the summaries\nprint(\"Computing RSA...\")\nresults = compute_rsa(summaries, model, tokenizer, device)\nresults = {\"results\": results} # wrap the results in a dictionary\n\nresults[\"metadata/reranking_model\"] = model_name\nresults[\"metadata/rsa_iterations\"] = 3\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:19:33.769401Z","iopub.execute_input":"2025-02-14T13:19:33.769728Z","iopub.status.idle":"2025-02-14T13:22:23.851731Z","shell.execute_reply.started":"2025-02-14T13:19:33.769697Z","shell.execute_reply":"2025-02-14T13:22:23.850675Z"}},"outputs":[{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc11eb7935b24826b8ddc4d94e7bbf3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bdcba2910ca4cc6aae3d9c35a718dae"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad4f4976ba074e11bb88596dee03013c"}},"metadata":{}},{"name":"stdout","text":"Loading Pegasus Tokenizer\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ed68536b4f456d99a7609c4b5ef246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2351cae4301246da918a42669c5a31ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a39601ece3a4438ba7581642eacb4253"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e1c77b16d74ae7a5e6785f88f9a67f"}},"metadata":{}},{"name":"stdout","text":"Model loaded\nModel to device\nSummaries loaded\nComputing RSA...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"('Audi',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n  7%|▋         | 1/15 [00:01<00:15,  1.12s/it]\u001b[A\n 20%|██        | 3/15 [00:01<00:07,  1.63it/s]\u001b[A\n 27%|██▋       | 4/15 [00:02<00:07,  1.57it/s]\u001b[A\n 33%|███▎      | 5/15 [00:03<00:06,  1.52it/s]\u001b[A\n 40%|████      | 6/15 [00:04<00:07,  1.26it/s]\u001b[A\n 47%|████▋     | 7/15 [00:05<00:06,  1.19it/s]\u001b[A\n 53%|█████▎    | 8/15 [00:06<00:05,  1.30it/s]\u001b[A\n 60%|██████    | 9/15 [00:06<00:04,  1.33it/s]\u001b[A\n 67%|██████▋   | 10/15 [00:07<00:03,  1.40it/s]\u001b[A\n 73%|███████▎  | 11/15 [00:08<00:02,  1.34it/s]\u001b[A\n 80%|████████  | 12/15 [00:09<00:02,  1.15it/s]\u001b[A\n 87%|████████▋ | 13/15 [00:10<00:01,  1.09it/s]\u001b[A\n 93%|█████████▎| 14/15 [00:11<00:00,  1.05it/s]\u001b[A\n100%|██████████| 15/15 [00:12<00:00,  1.22it/s]\u001b[A\n 10%|█         | 1/10 [00:12<01:53, 12.56s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 46]) listener probabilities\nI have (46,) consensuality scores\nI have 46 candidates\nI have (46,) uniqueness scores\nI have 46 candidates\n('Avanti Travel Insurance',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n 18%|█▊        | 2/11 [00:00<00:03,  2.62it/s]\u001b[A\n 27%|██▋       | 3/11 [00:01<00:04,  1.82it/s]\u001b[A\n 36%|███▋      | 4/11 [00:02<00:04,  1.69it/s]\u001b[A\n 45%|████▌     | 5/11 [00:02<00:03,  1.64it/s]\u001b[A\n 55%|█████▍    | 6/11 [00:03<00:03,  1.64it/s]\u001b[A\n 64%|██████▎   | 7/11 [00:04<00:02,  1.62it/s]\u001b[A\n 73%|███████▎  | 8/11 [00:04<00:01,  1.63it/s]\u001b[A\n 82%|████████▏ | 9/11 [00:05<00:01,  1.54it/s]\u001b[A\n 91%|█████████ | 10/11 [00:06<00:00,  1.49it/s]\u001b[A\n100%|██████████| 11/11 [00:06<00:00,  1.60it/s]\u001b[A\n 20%|██        | 2/10 [00:19<01:16,  9.51s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 35]) listener probabilities\nI have (35,) consensuality scores\nI have 35 candidates\nI have (35,) uniqueness scores\nI have 35 candidates\n('Danske Bank',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:04,  1.94it/s]\u001b[A\n 30%|███       | 3/10 [00:01<00:03,  1.93it/s]\u001b[A\n 40%|████      | 4/10 [00:02<00:03,  1.65it/s]\u001b[A\n 50%|█████     | 5/10 [00:03<00:03,  1.51it/s]\u001b[A\n 60%|██████    | 6/10 [00:03<00:02,  1.56it/s]\u001b[A\n 70%|███████   | 7/10 [00:04<00:01,  1.59it/s]\u001b[A\n 80%|████████  | 8/10 [00:05<00:01,  1.48it/s]\u001b[A\n 90%|█████████ | 9/10 [00:05<00:00,  1.41it/s]\u001b[A\n100%|██████████| 10/10 [00:06<00:00,  1.55it/s]\u001b[A\n 30%|███       | 3/10 [00:26<00:57,  8.19s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 30]) listener probabilities\nI have (30,) consensuality scores\nI have 30 candidates\nI have (30,) uniqueness scores\nI have 30 candidates\n('Google',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n  7%|▋         | 1/15 [00:00<00:01,  9.02it/s]\u001b[A\n 13%|█▎        | 2/15 [00:01<00:07,  1.65it/s]\u001b[A\n 20%|██        | 3/15 [00:02<00:10,  1.18it/s]\u001b[A\n 27%|██▋       | 4/15 [00:03<00:09,  1.14it/s]\u001b[A\n 33%|███▎      | 5/15 [00:03<00:07,  1.37it/s]\u001b[A\n 40%|████      | 6/15 [00:04<00:06,  1.48it/s]\u001b[A\n 47%|████▋     | 7/15 [00:04<00:05,  1.48it/s]\u001b[A\n 53%|█████▎    | 8/15 [00:05<00:04,  1.49it/s]\u001b[A\n 60%|██████    | 9/15 [00:06<00:03,  1.53it/s]\u001b[A\n 67%|██████▋   | 10/15 [00:06<00:03,  1.46it/s]\u001b[A\n 73%|███████▎  | 11/15 [00:07<00:02,  1.41it/s]\u001b[A\n 80%|████████  | 12/15 [00:08<00:01,  1.57it/s]\u001b[A\n 87%|████████▋ | 13/15 [00:08<00:01,  1.53it/s]\u001b[A\n 93%|█████████▎| 14/15 [00:09<00:00,  1.50it/s]\u001b[A\n100%|██████████| 15/15 [00:10<00:00,  1.48it/s]\u001b[A\n 40%|████      | 4/10 [00:36<00:53,  8.96s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 45]) listener probabilities\nI have (45,) consensuality scores\nI have 45 candidates\nI have (45,) uniqueness scores\nI have 45 candidates\n('IKEA',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:00<00:03,  4.89it/s]\u001b[A\n 10%|█         | 2/20 [00:01<00:11,  1.64it/s]\u001b[A\n 15%|█▌        | 3/20 [00:02<00:16,  1.03it/s]\u001b[A\n 20%|██        | 4/20 [00:03<00:16,  1.02s/it]\u001b[A\n 25%|██▌       | 5/20 [00:04<00:16,  1.09s/it]\u001b[A\n 30%|███       | 6/20 [00:05<00:13,  1.01it/s]\u001b[A\n 35%|███▌      | 7/20 [00:06<00:12,  1.04it/s]\u001b[A\n 40%|████      | 8/20 [00:07<00:10,  1.17it/s]\u001b[A\n 45%|████▌     | 9/20 [00:07<00:09,  1.18it/s]\u001b[A\n 50%|█████     | 10/20 [00:08<00:07,  1.25it/s]\u001b[A\n 55%|█████▌    | 11/20 [00:09<00:08,  1.07it/s]\u001b[A\n 60%|██████    | 12/20 [00:10<00:07,  1.02it/s]\u001b[A\n 65%|██████▌   | 13/20 [00:12<00:07,  1.05s/it]\u001b[A\n 70%|███████   | 14/20 [00:12<00:05,  1.09it/s]\u001b[A\n 75%|███████▌  | 15/20 [00:13<00:04,  1.11it/s]\u001b[A\n 80%|████████  | 16/20 [00:14<00:03,  1.15it/s]\u001b[A\n 85%|████████▌ | 17/20 [00:15<00:02,  1.14it/s]\u001b[A\n 90%|█████████ | 18/20 [00:16<00:01,  1.24it/s]\u001b[A\n 95%|█████████▌| 19/20 [00:17<00:00,  1.03it/s]\u001b[A\n100%|██████████| 20/20 [00:18<00:00,  1.08it/s]\u001b[A\n 50%|█████     | 5/10 [00:56<01:03, 12.74s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 63]) listener probabilities\nI have (63,) consensuality scores\nI have 63 candidates\nI have (63,) uniqueness scores\nI have 63 candidates\n('Just Eat',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n 12%|█▏        | 2/17 [00:01<00:07,  1.95it/s]\u001b[A\n 18%|█▊        | 3/17 [00:01<00:08,  1.67it/s]\u001b[A\n 24%|██▎       | 4/17 [00:02<00:09,  1.33it/s]\u001b[A\n 29%|██▉       | 5/17 [00:03<00:08,  1.34it/s]\u001b[A\n 35%|███▌      | 6/17 [00:04<00:09,  1.17it/s]\u001b[A\n 41%|████      | 7/17 [00:05<00:09,  1.08it/s]\u001b[A\n 47%|████▋     | 8/17 [00:06<00:07,  1.13it/s]\u001b[A\n 53%|█████▎    | 9/17 [00:07<00:07,  1.07it/s]\u001b[A\n 59%|█████▉    | 10/17 [00:08<00:05,  1.18it/s]\u001b[A\n 65%|██████▍   | 11/17 [00:09<00:05,  1.14it/s]\u001b[A\n 71%|███████   | 12/17 [00:10<00:04,  1.12it/s]\u001b[A\n 76%|███████▋  | 13/17 [00:11<00:03,  1.05it/s]\u001b[A\n 82%|████████▏ | 14/17 [00:12<00:03,  1.08s/it]\u001b[A\n 88%|████████▊ | 15/17 [00:13<00:02,  1.04s/it]\u001b[A\n 94%|█████████▍| 16/17 [00:14<00:00,  1.09it/s]\u001b[A\n100%|██████████| 17/17 [00:14<00:00,  1.14it/s]\u001b[A\n 60%|██████    | 6/10 [01:11<00:54, 13.54s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 52]) listener probabilities\nI have (52,) consensuality scores\nI have 52 candidates\nI have (52,) uniqueness scores\nI have 52 candidates\n('Lidl GB',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n 12%|█▎        | 2/16 [00:00<00:06,  2.01it/s]\u001b[A\n 19%|█▉        | 3/16 [00:02<00:09,  1.32it/s]\u001b[A\n 25%|██▌       | 4/16 [00:03<00:09,  1.21it/s]\u001b[A\n 31%|███▏      | 5/16 [00:04<00:10,  1.05it/s]\u001b[A\n 38%|███▊      | 6/16 [00:05<00:10,  1.02s/it]\u001b[A\n 44%|████▍     | 7/16 [00:06<00:08,  1.10it/s]\u001b[A\n 50%|█████     | 8/16 [00:06<00:07,  1.12it/s]\u001b[A\n 56%|█████▋    | 9/16 [00:08<00:07,  1.00s/it]\u001b[A\n 62%|██████▎   | 10/16 [00:09<00:06,  1.01s/it]\u001b[A\n 69%|██████▉   | 11/16 [00:10<00:06,  1.21s/it]\u001b[A\n 75%|███████▌  | 12/16 [00:12<00:05,  1.37s/it]\u001b[A\n 81%|████████▏ | 13/16 [00:13<00:03,  1.21s/it]\u001b[A\n 88%|████████▊ | 14/16 [00:14<00:02,  1.21s/it]\u001b[A\n 94%|█████████▍| 15/16 [00:15<00:01,  1.07s/it]\u001b[A\n100%|██████████| 16/16 [00:16<00:00,  1.03s/it]\u001b[A\n 70%|███████   | 7/10 [01:28<00:44, 14.73s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 50]) listener probabilities\nI have (50,) consensuality scores\nI have 50 candidates\nI have (50,) uniqueness scores\nI have 50 candidates\n('Perfume Click',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n 18%|█▊        | 2/11 [00:00<00:04,  2.13it/s]\u001b[A\n 27%|██▋       | 3/11 [00:01<00:04,  1.62it/s]\u001b[A\n 36%|███▋      | 4/11 [00:02<00:05,  1.28it/s]\u001b[A\n 45%|████▌     | 5/11 [00:03<00:05,  1.11it/s]\u001b[A\n 55%|█████▍    | 6/11 [00:04<00:04,  1.06it/s]\u001b[A\n 64%|██████▎   | 7/11 [00:06<00:04,  1.00s/it]\u001b[A\n 73%|███████▎  | 8/11 [00:06<00:02,  1.05it/s]\u001b[A\n 82%|████████▏ | 9/11 [00:07<00:01,  1.06it/s]\u001b[A\n 91%|█████████ | 10/11 [00:08<00:00,  1.01it/s]\u001b[A\n100%|██████████| 11/11 [00:09<00:00,  1.10it/s]\u001b[A\n 80%|████████  | 8/10 [01:38<00:26, 13.31s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 33]) listener probabilities\nI have (33,) consensuality scores\nI have 33 candidates\nI have (33,) uniqueness scores\nI have 33 candidates\n('The LEGO Group',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n 10%|█         | 2/20 [00:00<00:07,  2.36it/s]\u001b[A\n 15%|█▌        | 3/20 [00:02<00:13,  1.25it/s]\u001b[A\n 20%|██        | 4/20 [00:03<00:16,  1.05s/it]\u001b[A\n 25%|██▌       | 5/20 [00:04<00:16,  1.12s/it]\u001b[A\n 30%|███       | 6/20 [00:05<00:13,  1.01it/s]\u001b[A\n 35%|███▌      | 7/20 [00:06<00:12,  1.04it/s]\u001b[A\n 40%|████      | 8/20 [00:07<00:11,  1.01it/s]\u001b[A\n 45%|████▌     | 9/20 [00:08<00:11,  1.00s/it]\u001b[A\n 50%|█████     | 10/20 [00:09<00:08,  1.13it/s]\u001b[A\n 55%|█████▌    | 11/20 [00:10<00:09,  1.08s/it]\u001b[A\n 60%|██████    | 12/20 [00:12<00:09,  1.21s/it]\u001b[A\n 65%|██████▌   | 13/20 [00:13<00:09,  1.31s/it]\u001b[A\n 70%|███████   | 14/20 [00:14<00:06,  1.14s/it]\u001b[A\n 75%|███████▌  | 15/20 [00:15<00:06,  1.22s/it]\u001b[A\n 80%|████████  | 16/20 [00:17<00:05,  1.29s/it]\u001b[A\n 85%|████████▌ | 17/20 [00:18<00:04,  1.35s/it]\u001b[A\n 90%|█████████ | 18/20 [00:20<00:02,  1.27s/it]\u001b[A\n 95%|█████████▌| 19/20 [00:21<00:01,  1.20s/it]\u001b[A\n100%|██████████| 20/20 [00:21<00:00,  1.09s/it]\u001b[A\n 90%|█████████ | 9/10 [02:00<00:16, 16.04s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 62]) listener probabilities\nI have (62,) consensuality scores\nI have 62 candidates\nI have (62,) uniqueness scores\nI have 62 candidates\n('UberEATS',)\nhello this is the test version!\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n 11%|█         | 2/18 [00:00<00:07,  2.03it/s]\u001b[A\n 17%|█▋        | 3/18 [00:02<00:10,  1.36it/s]\u001b[A\n 22%|██▏       | 4/18 [00:02<00:10,  1.31it/s]\u001b[A\n 28%|██▊       | 5/18 [00:03<00:11,  1.17it/s]\u001b[A\n 33%|███▎      | 6/18 [00:04<00:09,  1.21it/s]\u001b[A\n 39%|███▉      | 7/18 [00:05<00:09,  1.15it/s]\u001b[A\n 44%|████▍     | 8/18 [00:06<00:09,  1.10it/s]\u001b[A\n 50%|█████     | 9/18 [00:07<00:09,  1.02s/it]\u001b[A\n 56%|█████▌    | 10/18 [00:09<00:08,  1.10s/it]\u001b[A\n 61%|██████    | 11/18 [00:09<00:06,  1.00it/s]\u001b[A\n 67%|██████▋   | 12/18 [00:10<00:05,  1.03it/s]\u001b[A\n 72%|███████▏  | 13/18 [00:11<00:04,  1.11it/s]\u001b[A\n 78%|███████▊  | 14/18 [00:12<00:03,  1.09it/s]\u001b[A\n 83%|████████▎ | 15/18 [00:13<00:02,  1.09it/s]\u001b[A\n 89%|████████▉ | 16/18 [00:14<00:01,  1.10it/s]\u001b[A\n 94%|█████████▍| 17/18 [00:15<00:01,  1.14s/it]\u001b[A\n100%|██████████| 18/18 [00:17<00:00,  1.02it/s]\u001b[A\n100%|██████████| 10/10 [02:19<00:00, 13.91s/it]","output_type":"stream"},{"name":"stdout","text":"I have torch.Size([10, 56]) listener probabilities\nI have (56,) consensuality scores\nI have 56 candidates\nI have (56,) uniqueness scores\nI have 56 candidates\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"summaries","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:23.852650Z","iopub.execute_input":"2025-02-14T13:22:23.852956Z","iopub.status.idle":"2025-02-14T13:22:23.869693Z","shell.execute_reply.started":"2025-02-14T13:22:23.852919Z","shell.execute_reply":"2025-02-14T13:22:23.868944Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"     index           id                                               text  \\\n0        0  Danske Bank  Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...   \n1        0  Danske Bank  Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...   \n2        0  Danske Bank  Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...   \n3        0  Danske Bank  Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...   \n4        0  Danske Bank  Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...   \n..     ...          ...                                                ...   \n469     99     UberEATS  Absolutely atrocious customer service - they s...   \n470     99     UberEATS  Absolutely atrocious customer service - they s...   \n471     99     UberEATS  Absolutely atrocious customer service - they s...   \n472     99     UberEATS  Absolutely atrocious customer service - they s...   \n473     99     UberEATS  Absolutely atrocious customer service - they s...   \n\n                                                  gold  \\\n0    Customers are generally very satisfied with th...   \n1    Customers are generally very satisfied with th...   \n2    Customers are generally very satisfied with th...   \n3    Customers are generally very satisfied with th...   \n4    Customers are generally very satisfied with th...   \n..                                                 ...   \n469  Customers express widespread dissatisfaction w...   \n470  Customers express widespread dissatisfaction w...   \n471  Customers express widespread dissatisfaction w...   \n472  Customers express widespread dissatisfaction w...   \n473  Customers express widespread dissatisfaction w...   \n\n                                               summary  id_candidate  \n0    Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...             0  \n1    However, my recent experience has been frustra...             1  \n2    I needed a temporary overdraft extension of ju...             2  \n3    What should have been a simple request turned ...             3  \n4    Each representative seemed disconnected, and I...             4  \n..                                                 ...           ...  \n469  And I don&apos;t mean the drivers who have bee...             4  \n470                                        Kafkaesque.             5  \n471  Just Eat is infinitely better, as is Foodhub -...             6  \n472                       No chat, no phone, no email.             7  \n473  I&apos;m putting a complaint in to Trading Sta...             8  \n\n[474 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>text</th>\n      <th>gold</th>\n      <th>summary</th>\n      <th>id_candidate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Danske Bank</td>\n      <td>Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...</td>\n      <td>Customers are generally very satisfied with th...</td>\n      <td>Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Danske Bank</td>\n      <td>Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...</td>\n      <td>Customers are generally very satisfied with th...</td>\n      <td>However, my recent experience has been frustra...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>Danske Bank</td>\n      <td>Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...</td>\n      <td>Customers are generally very satisfied with th...</td>\n      <td>I needed a temporary overdraft extension of ju...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>Danske Bank</td>\n      <td>Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...</td>\n      <td>Customers are generally very satisfied with th...</td>\n      <td>What should have been a simple request turned ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Danske Bank</td>\n      <td>Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...</td>\n      <td>Customers are generally very satisfied with th...</td>\n      <td>Each representative seemed disconnected, and I...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>469</th>\n      <td>99</td>\n      <td>UberEATS</td>\n      <td>Absolutely atrocious customer service - they s...</td>\n      <td>Customers express widespread dissatisfaction w...</td>\n      <td>And I don&amp;apos;t mean the drivers who have bee...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>470</th>\n      <td>99</td>\n      <td>UberEATS</td>\n      <td>Absolutely atrocious customer service - they s...</td>\n      <td>Customers express widespread dissatisfaction w...</td>\n      <td>Kafkaesque.</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>471</th>\n      <td>99</td>\n      <td>UberEATS</td>\n      <td>Absolutely atrocious customer service - they s...</td>\n      <td>Customers express widespread dissatisfaction w...</td>\n      <td>Just Eat is infinitely better, as is Foodhub -...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>472</th>\n      <td>99</td>\n      <td>UberEATS</td>\n      <td>Absolutely atrocious customer service - they s...</td>\n      <td>Customers express widespread dissatisfaction w...</td>\n      <td>No chat, no phone, no email.</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>473</th>\n      <td>99</td>\n      <td>UberEATS</td>\n      <td>Absolutely atrocious customer service - they s...</td>\n      <td>Customers express widespread dissatisfaction w...</td>\n      <td>I&amp;apos;m putting a complaint in to Trading Sta...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>474 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"df_results = pd.DataFrame(results[\"results\"])\ndf_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:23.870775Z","iopub.execute_input":"2025-02-14T13:22:23.871106Z","iopub.status.idle":"2025-02-14T13:22:24.585478Z","shell.execute_reply.started":"2025-02-14T13:22:23.871077Z","shell.execute_reply":"2025-02-14T13:22:24.584464Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                           id  \\\n0                     (Audi,)   \n1  (Avanti Travel Insurance,)   \n2              (Danske Bank,)   \n3                   (Google,)   \n4                     (IKEA,)   \n5                 (Just Eat,)   \n6                  (Lidl GB,)   \n7            (Perfume Click,)   \n8           (The LEGO Group,)   \n9                 (UberEATS,)   \n\n                                            best_rsa  \\\n0  [They didn’t leave the wheel nut lock key in t...   \n1  [Just renewed my holiday insurance with Avanti...   \n2  [Due to this impersonal and inefficient servic...   \n3  [I highly recommend you don’t either., We have...   \n4  [Had a few issues to get sorted so call ikea h...   \n5  [Do not use., My order was not delivered so I ...   \n6  [The customer service  at this store is excell...   \n7  [this company is more than happy to rip you of...   \n8  [‘What are the codes’….., They had the AUDACIT...   \n9  [I contacted uber support, and they told me to...   \n\n                                           best_base  \\\n0  [They didn’t leave the wheel nut lock key in t...   \n1  [Just renewed my holiday insurance with Avanti...   \n2  [Due to this impersonal and inefficient servic...   \n3  [I highly recommend you don’t either., We have...   \n4  [Had a few issues to get sorted so call ikea h...   \n5  [Do not use., They don’t deserve 1* but wouldn...   \n6  [The customer service  at this store is excell...   \n7  [this company is more than happy to rip you of...   \n8  [‘What are the codes’….., They had the AUDACIT...   \n9  [I contacted uber support, and they told me to...   \n\n                                     best_uniqueness  \\\n0  [I hope to own one from this globally successf...   \n1  [Having received an on-line quote I was unable...   \n2  [I needed a temporary overdraft extension of j...   \n3  [Google itself acknowledges the issue and even...   \n4  [To add had a lovely conversation with her at ...   \n5  [This company shouldn’t be trading!, No rxplan...   \n6  [Not convinced about the quality of some of th...   \n7  [Thank you for such a wide range available, I ...   \n8  [Sarah Watkins, Missing a brick from bag 13 of...   \n9  [which I honestly think they didn&apos;t do., ...   \n\n                                          speaker_df  \\\n0                                                ...   \n1                                                ...   \n2                                                ...   \n3                                                ...   \n4                                                ...   \n5                                                ...   \n6                                                ...   \n7                                                ...   \n8                                                ...   \n9                                                ...   \n\n                                         listener_df  \\\n0                                                ...   \n1                                                ...   \n2                                                ...   \n3                                                ...   \n4                                                ...   \n5                                                ...   \n6                                                ...   \n7                                                ...   \n8                                                ...   \n9                                                ...   \n\n                                    initial_listener  \\\n0                                                ...   \n1                                                ...   \n2                                                ...   \n3                                                ...   \n4                                                ...   \n5                                                ...   \n6                                                ...   \n7                                                ...   \n8                                                ...   \n9                                                ...   \n\n                             language_model_proba_df  \\\n0                                                ...   \n1                                                ...   \n2                                                ...   \n3                                                ...   \n4                                                ...   \n5                                                ...   \n6                                                ...   \n7                                                ...   \n8                                                ...   \n9                                                ...   \n\n                        initial_consensuality_scores  \\\n0  Audi is a car like no other.                  ...   \n1  Just renewed my holiday insurance with Avanti ...   \n2  Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...   \n3  My Google Pixel 8 developed screen issues 9 mo...   \n4  Had a few issues to get sorted so call ikea he...   \n5  They don’t deserve 1* but wouldn’t allow less....   \n6  The customer service  at this store is excelle...   \n7  I love my purchases that arrived by the time i...   \n8  Missing a brick from bag 13 of Lego Titanic.  ...   \n9  Please don&apos;t order from restaurants that ...   \n\n                                consensuality_scores  \\\n0  Audi is a car like no other.                  ...   \n1  Just renewed my holiday insurance with Avanti ...   \n2  Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...   \n3  My Google Pixel 8 developed screen issues 9 mo...   \n4  Had a few issues to get sorted so call ikea he...   \n5  They don’t deserve 1* but wouldn’t allow less....   \n6  The customer service  at this store is excelle...   \n7  I love my purchases that arrived by the time i...   \n8  Missing a brick from bag 13 of Lego Titanic.  ...   \n9  Please don&apos;t order from restaurants that ...   \n\n                                   uniqueness_scores  \\\n0  Audi is a car like no other.                  ...   \n1  Just renewed my holiday insurance with Avanti ...   \n2  Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...   \n3  My Google Pixel 8 developed screen issues 9 mo...   \n4  Had a few issues to get sorted so call ikea he...   \n5  They don’t deserve 1* but wouldn’t allow less....   \n6  The customer service  at this store is excelle...   \n7  I love my purchases that arrived by the time i...   \n8  Missing a brick from bag 13 of Lego Titanic.  ...   \n9  Please don&apos;t order from restaurants that ...   \n\n                                                gold  rationality  \\\n0  Customers are generally very satisfied with th...            3   \n1  Customers are generally very satisfied with Av...            3   \n2  Customers are generally very satisfied with th...            3   \n3  Customers have expressed mixed sentiments abou...            3   \n4  Customers are largely dissatisfied with their ...            3   \n5  Customers are largely dissatisfied with Just E...            3   \n6  Customers are largely dissatisfied with this b...            3   \n7  Customers are extremely satisfied with this co...            3   \n8  Customers are generally unhappy with their exp...            3   \n9  Customers express widespread dissatisfaction w...            3   \n\n                                     text_candidates  \n0       index    id                              ...  \n1       index                       id  \\\n383    ...  \n2      index           id                        ...  \n3       index      id                            ...  \n4       index    id                              ...  \n5       index        id                          ...  \n6       index       id                           ...  \n7       index             id                     ...  \n8      index              id                     ...  \n9       index        id                          ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>best_rsa</th>\n      <th>best_base</th>\n      <th>best_uniqueness</th>\n      <th>speaker_df</th>\n      <th>listener_df</th>\n      <th>initial_listener</th>\n      <th>language_model_proba_df</th>\n      <th>initial_consensuality_scores</th>\n      <th>consensuality_scores</th>\n      <th>uniqueness_scores</th>\n      <th>gold</th>\n      <th>rationality</th>\n      <th>text_candidates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(Audi,)</td>\n      <td>[They didn’t leave the wheel nut lock key in t...</td>\n      <td>[They didn’t leave the wheel nut lock key in t...</td>\n      <td>[I hope to own one from this globally successf...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>Audi is a car like no other.                  ...</td>\n      <td>Audi is a car like no other.                  ...</td>\n      <td>Audi is a car like no other.                  ...</td>\n      <td>Customers are generally very satisfied with th...</td>\n      <td>3</td>\n      <td>index    id                              ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(Avanti Travel Insurance,)</td>\n      <td>[Just renewed my holiday insurance with Avanti...</td>\n      <td>[Just renewed my holiday insurance with Avanti...</td>\n      <td>[Having received an on-line quote I was unable...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>Just renewed my holiday insurance with Avanti ...</td>\n      <td>Just renewed my holiday insurance with Avanti ...</td>\n      <td>Just renewed my holiday insurance with Avanti ...</td>\n      <td>Customers are generally very satisfied with Av...</td>\n      <td>3</td>\n      <td>index                       id  \\\n383    ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(Danske Bank,)</td>\n      <td>[Due to this impersonal and inefficient servic...</td>\n      <td>[Due to this impersonal and inefficient servic...</td>\n      <td>[I needed a temporary overdraft extension of j...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...</td>\n      <td>Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...</td>\n      <td>Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a...</td>\n      <td>Customers are generally very satisfied with th...</td>\n      <td>3</td>\n      <td>index           id                        ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(Google,)</td>\n      <td>[I highly recommend you don’t either., We have...</td>\n      <td>[I highly recommend you don’t either., We have...</td>\n      <td>[Google itself acknowledges the issue and even...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>My Google Pixel 8 developed screen issues 9 mo...</td>\n      <td>My Google Pixel 8 developed screen issues 9 mo...</td>\n      <td>My Google Pixel 8 developed screen issues 9 mo...</td>\n      <td>Customers have expressed mixed sentiments abou...</td>\n      <td>3</td>\n      <td>index      id                            ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(IKEA,)</td>\n      <td>[Had a few issues to get sorted so call ikea h...</td>\n      <td>[Had a few issues to get sorted so call ikea h...</td>\n      <td>[To add had a lovely conversation with her at ...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>Had a few issues to get sorted so call ikea he...</td>\n      <td>Had a few issues to get sorted so call ikea he...</td>\n      <td>Had a few issues to get sorted so call ikea he...</td>\n      <td>Customers are largely dissatisfied with their ...</td>\n      <td>3</td>\n      <td>index    id                              ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(Just Eat,)</td>\n      <td>[Do not use., My order was not delivered so I ...</td>\n      <td>[Do not use., They don’t deserve 1* but wouldn...</td>\n      <td>[This company shouldn’t be trading!, No rxplan...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>They don’t deserve 1* but wouldn’t allow less....</td>\n      <td>They don’t deserve 1* but wouldn’t allow less....</td>\n      <td>They don’t deserve 1* but wouldn’t allow less....</td>\n      <td>Customers are largely dissatisfied with Just E...</td>\n      <td>3</td>\n      <td>index        id                          ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(Lidl GB,)</td>\n      <td>[The customer service  at this store is excell...</td>\n      <td>[The customer service  at this store is excell...</td>\n      <td>[Not convinced about the quality of some of th...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>The customer service  at this store is excelle...</td>\n      <td>The customer service  at this store is excelle...</td>\n      <td>The customer service  at this store is excelle...</td>\n      <td>Customers are largely dissatisfied with this b...</td>\n      <td>3</td>\n      <td>index       id                           ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(Perfume Click,)</td>\n      <td>[this company is more than happy to rip you of...</td>\n      <td>[this company is more than happy to rip you of...</td>\n      <td>[Thank you for such a wide range available, I ...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>I love my purchases that arrived by the time i...</td>\n      <td>I love my purchases that arrived by the time i...</td>\n      <td>I love my purchases that arrived by the time i...</td>\n      <td>Customers are extremely satisfied with this co...</td>\n      <td>3</td>\n      <td>index             id                     ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(The LEGO Group,)</td>\n      <td>[‘What are the codes’….., They had the AUDACIT...</td>\n      <td>[‘What are the codes’….., They had the AUDACIT...</td>\n      <td>[Sarah Watkins, Missing a brick from bag 13 of...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>Missing a brick from bag 13 of Lego Titanic.  ...</td>\n      <td>Missing a brick from bag 13 of Lego Titanic.  ...</td>\n      <td>Missing a brick from bag 13 of Lego Titanic.  ...</td>\n      <td>Customers are generally unhappy with their exp...</td>\n      <td>3</td>\n      <td>index              id                     ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>(UberEATS,)</td>\n      <td>[I contacted uber support, and they told me to...</td>\n      <td>[I contacted uber support, and they told me to...</td>\n      <td>[which I honestly think they didn&amp;apos;t do., ...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>Please don&amp;apos;t order from restaurants that ...</td>\n      <td>Please don&amp;apos;t order from restaurants that ...</td>\n      <td>Please don&amp;apos;t order from restaurants that ...</td>\n      <td>Customers express widespread dissatisfaction w...</td>\n      <td>3</td>\n      <td>index        id                          ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"for entry in results[\"results\"]:\n    print(f\"Documento ID: {entry['id']}\")\n    print(f\"Riassunto Generato (best_rsa): {entry['best_rsa']}\")\n    print(f\"Riassunto Gold: {entry['gold']}\")\n    print(f\"Punteggio RSA: {entry['best_rsa']}\")\n    print(\"-\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:24.589564Z","iopub.execute_input":"2025-02-14T13:22:24.589850Z","iopub.status.idle":"2025-02-14T13:22:24.604620Z","shell.execute_reply.started":"2025-02-14T13:22:24.589818Z","shell.execute_reply":"2025-02-14T13:22:24.600114Z"}},"outputs":[{"name":"stdout","text":"Documento ID: ('Audi',)\nRiassunto Generato (best_rsa): ['They didn’t leave the wheel nut lock key in the car and the service centre doesn’t answer the phone and main number just keeps putting through to them and said the best thing is for me to go and collect it in person - so I have to waste at least 45 minutes of my time to do a round trip because they hadn’t returned this routine item.'\n 'Very professional service!'\n 'I purchased a A1 from this dealership which from the outset was faulty.'\n 'Highly recommend.' 'Over 2 years later still awaiting both.'\n 'In July 2024, I sold my Audi Q5 bought in 2015 in Abu Dhabi.'\n 'Visited many different branches and the guys here were the best by far.'\n 'coolant water tank, stench in the cabin emanating from the engine space, etc.'\n 'Shout out to Sam, Hannah and Jimmy.'\n 'The introduction was sufficient but the Audi connect and app could not be set.']\nRiassunto Gold: Customers are generally very satisfied with this company. \n\nCustomers mention that the service provided by Audi is excellent. They find the cars to be comfortable, well-built, and enjoyable to drive. They appreciate the support offered by the company after purchase and are pleased with the service they receive during routine maintenance.  Customers also share positive feedback about the customer service, describing interactions with staff as helpful, professional, and efficient.  However,  some customers express concerns about the customer service experience at certain dealerships, describing encounters as unhelpful, unprofessional, and  disappointing. They find communication to be challenging and express frustration with delays in service and a lack of responsiveness to their concerns. \n\nPunteggio RSA: ['They didn’t leave the wheel nut lock key in the car and the service centre doesn’t answer the phone and main number just keeps putting through to them and said the best thing is for me to go and collect it in person - so I have to waste at least 45 minutes of my time to do a round trip because they hadn’t returned this routine item.'\n 'Very professional service!'\n 'I purchased a A1 from this dealership which from the outset was faulty.'\n 'Highly recommend.' 'Over 2 years later still awaiting both.'\n 'In July 2024, I sold my Audi Q5 bought in 2015 in Abu Dhabi.'\n 'Visited many different branches and the guys here were the best by far.'\n 'coolant water tank, stench in the cabin emanating from the engine space, etc.'\n 'Shout out to Sam, Hannah and Jimmy.'\n 'The introduction was sufficient but the Audi connect and app could not be set.']\n--------------------------------------------------------------------------------\nDocumento ID: ('Avanti Travel Insurance',)\nRiassunto Generato (best_rsa): ['Just renewed my holiday insurance with Avanti spoke to a lovely lady about increase which she took the time to explain all aspects of the new amount and after searching for similar insurer I found I would not be covered for half as much.'\n 'Apparently it pays to read the small print.'\n 'I had researched cost and taken care when completing online form for myself and husband.'\n 'The assistant told me my quote had suddenly increased by over £200 despite it clearly stating &quot;the quote is valid for 30 days&quot;.'\n 'Following a long, involved and somewhat frustrating discussion this quote was eventually honoured.'\n 'Once again thanks Avanti 😊'\n '\\nI had such a positive experience speaking with the lady who assisted me with my travel insurance.'\n 'Let&apos;s be honest, the price is the make or break of whether you purchase or not.'\n 'I was fortunate to be able to talk this through with a lovely customer advisor, Lucy .'\n 'Many thanks']\nRiassunto Gold: Customers are generally very satisfied with Avanti. \n\nCustomers mention that the website is easy to navigate and use for obtaining quotes and purchasing insurance. Customers think that the prices offered by Avanti are competitive, especially for those with pre-existing medical conditions.  Customers highlight the helpfulness, knowledge, and efficiency of Avanti's customer service representatives, particularly when dealing with complex queries or policy adjustments. Customers appreciate the clear and concise explanations provided regarding coverage options and policy details. Some customers who have been with Avanti for several years express their continued satisfaction and have not had any issues with claims.  \n\nPunteggio RSA: ['Just renewed my holiday insurance with Avanti spoke to a lovely lady about increase which she took the time to explain all aspects of the new amount and after searching for similar insurer I found I would not be covered for half as much.'\n 'Apparently it pays to read the small print.'\n 'I had researched cost and taken care when completing online form for myself and husband.'\n 'The assistant told me my quote had suddenly increased by over £200 despite it clearly stating &quot;the quote is valid for 30 days&quot;.'\n 'Following a long, involved and somewhat frustrating discussion this quote was eventually honoured.'\n 'Once again thanks Avanti 😊'\n '\\nI had such a positive experience speaking with the lady who assisted me with my travel insurance.'\n 'Let&apos;s be honest, the price is the make or break of whether you purchase or not.'\n 'I was fortunate to be able to talk this through with a lovely customer advisor, Lucy .'\n 'Many thanks']\n--------------------------------------------------------------------------------\nDocumento ID: ('Danske Bank',)\nRiassunto Generato (best_rsa): ['Due to this impersonal and inefficient service, I&apos;ve decided to switch to the bank where I hold my business account.'\n 'Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a loyal customer of Danske Bank for over 40 years, I’ve always appreciated their reliability.'\n 'My Danske bank personal advisor was very helpful in taking the time to explore different financial scenarios to fit my needs with the home loan.'\n 'Each representative seemed disconnected, and I felt more like a number than a valued customer.'\n 'I hope Danske Bank will improve their customer service to better support their long-term clients in the future.'\n 'I appreciated his time and personal approach.'\n 'I had the meeting with Mr. Kasper Weiskvist Jensen who is a very knowledgeable investment advisor, who was very helpful to answer may questions and gave various relevant advises thereof.'\n 'My Danske bank personal advisor was very helpful in taking the time to explore different financial scenarios to fit my needs with the home loan.'\n 'However, my recent experience has been frustrating.'\n 'I recently had an excellent experience with Danske Bank Ringsted branch, specifically with our advisor, Makolli.']\nRiassunto Gold: Customers are generally very satisfied with this company. \n\nCustomers mention that the service provided by Danske Bank is noteworthy and praise the bank's reliability. Customers share experiences with staff members who are described as knowledgeable, helpful, and professional. They express satisfaction with the advice and support received from their advisors, especially in areas such as investment and home loans. Customers appreciate the bank's efforts to understand their individual financial needs and provide personalized solutions. \n\nHowever, some customers express dissatisfaction with the customer service, stating that it can be challenging to get in touch with representatives and that responses can be slow. There are mentions of difficulties in tasks such as opening accounts or receiving requested cards, with some customers finding these processes to be time-consuming. \n\nPunteggio RSA: ['Due to this impersonal and inefficient service, I&apos;ve decided to switch to the bank where I hold my business account.'\n 'Review for Danske Bank\\n\\nRating: ★☆☆☆\\n\\nAs a loyal customer of Danske Bank for over 40 years, I’ve always appreciated their reliability.'\n 'My Danske bank personal advisor was very helpful in taking the time to explore different financial scenarios to fit my needs with the home loan.'\n 'Each representative seemed disconnected, and I felt more like a number than a valued customer.'\n 'I hope Danske Bank will improve their customer service to better support their long-term clients in the future.'\n 'I appreciated his time and personal approach.'\n 'I had the meeting with Mr. Kasper Weiskvist Jensen who is a very knowledgeable investment advisor, who was very helpful to answer may questions and gave various relevant advises thereof.'\n 'My Danske bank personal advisor was very helpful in taking the time to explore different financial scenarios to fit my needs with the home loan.'\n 'However, my recent experience has been frustrating.'\n 'I recently had an excellent experience with Danske Bank Ringsted branch, specifically with our advisor, Makolli.']\n--------------------------------------------------------------------------------\nDocumento ID: ('Google',)\nRiassunto Generato (best_rsa): ['I highly recommend you don’t either.'\n 'We have just had a wood burner fitted by Nigel and Graham a wonderful experience from the very first meeting Nigel explained everything to the both of us approximately two weeks later our wood fire was installed,care was taken in our home a very professional job look really great and works.'\n 'Validation of Googles services can not be trusted as Gulf if Mexico is not Golf of Anerica, \\nYou change the truth to your covinience\\nAnd this is not valid data'\n 'We have just had a wood burner fitted by Nigel and Graham a wonderful experience from the very first meeting Nigel explained everything to the both of us approximately two weeks later our wood fire was installed,care was taken in our home a very professional job look really great and works.'\n 'Extremely well many thanks to a great term\\nMany thanks Brian and Sheila'\n 'My Google Pixel 8 developed screen issues 9 months after purchase.'\n 'We have just had a wood burner fitted by Nigel and Graham a wonderful experience from the very first meeting Nigel explained everything to the both of us approximately two weeks later our wood fire was installed,care was taken in our home a very professional job look really great and works.'\n 'I have lost work due to this'\n 'We have just had a wood burner fitted by Nigel and Graham a wonderful experience from the very first meeting Nigel explained everything to the both of us approximately two weeks later our wood fire was installed,care was taken in our home a very professional job look really great and works.'\n 'His professionalism and expertise was second to none .']\nRiassunto Gold: Customers have expressed mixed sentiments about their experiences with this company. \n\nWhile some customers are happy with the company's services, others have raised concerns regarding specific aspects of their interactions with the company. Customers mention encountering challenges when seeking support and receiving timely responses. There have also been mentions of billing issues, particularly regarding subscriptions and refunds. \n\nSome customers have also shared feedback on the company's products, with varying opinions on their reliability and performance. Some customers expressed dissatisfaction with product quality and reported encountering defects.  \n\nPunteggio RSA: ['I highly recommend you don’t either.'\n 'We have just had a wood burner fitted by Nigel and Graham a wonderful experience from the very first meeting Nigel explained everything to the both of us approximately two weeks later our wood fire was installed,care was taken in our home a very professional job look really great and works.'\n 'Validation of Googles services can not be trusted as Gulf if Mexico is not Golf of Anerica, \\nYou change the truth to your covinience\\nAnd this is not valid data'\n 'We have just had a wood burner fitted by Nigel and Graham a wonderful experience from the very first meeting Nigel explained everything to the both of us approximately two weeks later our wood fire was installed,care was taken in our home a very professional job look really great and works.'\n 'Extremely well many thanks to a great term\\nMany thanks Brian and Sheila'\n 'My Google Pixel 8 developed screen issues 9 months after purchase.'\n 'We have just had a wood burner fitted by Nigel and Graham a wonderful experience from the very first meeting Nigel explained everything to the both of us approximately two weeks later our wood fire was installed,care was taken in our home a very professional job look really great and works.'\n 'I have lost work due to this'\n 'We have just had a wood burner fitted by Nigel and Graham a wonderful experience from the very first meeting Nigel explained everything to the both of us approximately two weeks later our wood fire was installed,care was taken in our home a very professional job look really great and works.'\n 'His professionalism and expertise was second to none .']\n--------------------------------------------------------------------------------\nDocumento ID: ('IKEA',)\nRiassunto Generato (best_rsa): ['Had a few issues to get sorted so call ikea helpline today.'\n 'I waited on hold 2 hours between phone and chat.'\n 'To add had a lovely conversation with her at the end which made my day.'\n 'After talking with them back and forth, they finally ordered me parts.'\n 'Delayed delivery by two weeks.' 'Just an all round poor experience.'\n 'The stoves are in stock!' 'The charge for two was right there!'\n 'They do not provide a prepaid shipping label for returns.'\n 'Great service 👏 \\n\\nPhon']\nRiassunto Gold: Customers are largely dissatisfied with their experiences with this company. \n\nCustomers express significant frustration with the company's delivery service, citing frequent delays, missed deliveries, and a lack of communication regarding order status. Many customers report receiving damaged goods and encountering difficulties with returns and refunds. Customers also express dissatisfaction with the customer service department, citing unhelpful representatives, long wait times, and difficulty contacting management. There are also complaints about the online ordering process, with some customers reporting issues with inaccurate inventory information and difficulties obtaining refunds for out-of-stock items. Overall, customers perceive a decline in product quality and value for money. \n\nPunteggio RSA: ['Had a few issues to get sorted so call ikea helpline today.'\n 'I waited on hold 2 hours between phone and chat.'\n 'To add had a lovely conversation with her at the end which made my day.'\n 'After talking with them back and forth, they finally ordered me parts.'\n 'Delayed delivery by two weeks.' 'Just an all round poor experience.'\n 'The stoves are in stock!' 'The charge for two was right there!'\n 'They do not provide a prepaid shipping label for returns.'\n 'Great service 👏 \\n\\nPhon']\n--------------------------------------------------------------------------------\nDocumento ID: ('Just Eat',)\nRiassunto Generato (best_rsa): ['Do not use.'\n 'My order was not delivered so I contacted Just Eat for a refund.'\n 'Like don’t you already have my address ????'\n 'When restaurants have a one star food safety rating the are strongly advised to close.'\n 'Terrible customer service!' 'SHOCKING!!!!!!!!!!!' 'Not to us it didn’t.'\n '0 stars comparing to their competitors Deliveroo etc.'\n 'Surely will take a legal route.'\n 'Just Eat customer service team didn’t seem to take the time to look into the matter.']\nRiassunto Gold: Customers are largely dissatisfied with Just Eat based on recent experiences. \n\nCustomers mention that orders frequently arrive late, often cold or with items missing. Many customers express frustration with delivery drivers who fail to follow delivery instructions, such as delivering to the wrong address or not contacting the customer upon arrival. \n\nCustomers think that the customer service is unhelpful and unresponsive, with many stating that their complaints were dismissed or ignored. They express dissatisfaction with the refund process, claiming that it is difficult to receive a full refund, even when orders are not delivered or arrive with missing items. \n\nCustomers highlight issues with Just Eat's refund policies, citing instances where they were offered credit instead of a monetary refund. This has led some customers to believe that the company prioritizes profit over customer satisfaction. \n\nPunteggio RSA: ['Do not use.'\n 'My order was not delivered so I contacted Just Eat for a refund.'\n 'Like don’t you already have my address ????'\n 'When restaurants have a one star food safety rating the are strongly advised to close.'\n 'Terrible customer service!' 'SHOCKING!!!!!!!!!!!' 'Not to us it didn’t.'\n '0 stars comparing to their competitors Deliveroo etc.'\n 'Surely will take a legal route.'\n 'Just Eat customer service team didn’t seem to take the time to look into the matter.']\n--------------------------------------------------------------------------------\nDocumento ID: ('Lidl GB',)\nRiassunto Generato (best_rsa): ['The customer service  at this store is excellent I am so pleased I can contact customer service and any problem is sorted out straight away .unlike other stores this makes a massive difference .it makes me feel much better and I will always shop at this store because I feel cared for and listened well done !...and thankyou for really caring about your customers .'\n 'Lidl couldn&apos;t help unless you keep your receipts (like most people do ?!??).'\n 'Normally Tesco shoppers but thought we would give Lidl a try , however after two or three weeks went back to Tesco.'\n 'I&apos;ll consider starting my shop at Tesco&apos;s instead.'\n 'Normally Tesco shoppers but thought we would give Lidl a try , however after two or three weeks went back to Tesco.'\n 'Charged excessively for using the rapid charge station.'\n 'Normally Tesco shoppers but thought we would give Lidl a try , however after two or three weeks went back to Tesco.'\n 'We found the shopping experience not as good, no cafe, no clothes or household items and very limited choice and availability, we were having to still go to Tesco for items that weren&apos;t available in Lidl.'\n 'Whilst good value for money, on this occasion was charged twice for a £4 item and this is not the first time, fortunately I always ask for my receipt and check when through the till area, checkout operator was very appolgetic when l pointed this out.'\n 'Branch concerned was Woodhouse Lane Wigan.']\nRiassunto Gold: Customers are largely dissatisfied with this business. \n\nWhile specific details about customer experiences have not been provided, there seems to be a general dissatisfaction with the customer service. Customers mention that the staff can be unhelpful and impolite. There are also recurring concerns about product quality, with many customers expressing disappointment over expired or unsatisfactory items. It's worth noting that customers also mention issues with the pricing and in-store experience, including long queues, poorly stocked shelves, and problems with the Lidl app and its discount system. \n\nPunteggio RSA: ['The customer service  at this store is excellent I am so pleased I can contact customer service and any problem is sorted out straight away .unlike other stores this makes a massive difference .it makes me feel much better and I will always shop at this store because I feel cared for and listened well done !...and thankyou for really caring about your customers .'\n 'Lidl couldn&apos;t help unless you keep your receipts (like most people do ?!??).'\n 'Normally Tesco shoppers but thought we would give Lidl a try , however after two or three weeks went back to Tesco.'\n 'I&apos;ll consider starting my shop at Tesco&apos;s instead.'\n 'Normally Tesco shoppers but thought we would give Lidl a try , however after two or three weeks went back to Tesco.'\n 'Charged excessively for using the rapid charge station.'\n 'Normally Tesco shoppers but thought we would give Lidl a try , however after two or three weeks went back to Tesco.'\n 'We found the shopping experience not as good, no cafe, no clothes or household items and very limited choice and availability, we were having to still go to Tesco for items that weren&apos;t available in Lidl.'\n 'Whilst good value for money, on this occasion was charged twice for a £4 item and this is not the first time, fortunately I always ask for my receipt and check when through the till area, checkout operator was very appolgetic when l pointed this out.'\n 'Branch concerned was Woodhouse Lane Wigan.']\n--------------------------------------------------------------------------------\nDocumento ID: ('Perfume Click',)\nRiassunto Generato (best_rsa): ['this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'I chose to order from this company based on how many positive reviews they had.'\n 'I love my purchases that arrived by the time it was given when purchase was confirmed.'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!']\nRiassunto Gold: Customers are extremely satisfied with this company. \n\nCustomers mention that the delivery service is quick and efficient. They appreciate the prompt delivery and the care taken in packaging the products. Customers think that the website is easy to navigate and that the ordering process is smooth and straightforward. Customers love the wide range of products available and  find the prices to be very competitive. They are particularly pleased with the availability of popular brands and hard-to-find fragrances. Customers say that the customer service is responsive and helpful. They feel confident recommending this company to others. \n\nPunteggio RSA: ['this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'I chose to order from this company based on how many positive reviews they had.'\n 'I love my purchases that arrived by the time it was given when purchase was confirmed.'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!'\n 'this company is more than happy to rip you off and hide behind the fact that they need to &apos;investigate&apos;whether you have received your product..........i can send them photos of my doors which will show that it&apos;s totally different to their delivery photos......but still refuse to refund and leave me out of pocket.................terrible company and do not trust them............you have been warned!!!!']\n--------------------------------------------------------------------------------\nDocumento ID: ('The LEGO Group',)\nRiassunto Generato (best_rsa): ['‘What are the codes’…..'\n 'They had the AUDACITY to say we’ll send them for free!!'\n 'I’ve already paid over the odds for bits of plastic!!'\n 'I was greeted as I entered the store which was a pleasant start to the experience.'\n 'Never again.' 'Lego is already WAY OVERPRICED!!' 'Still not arrived.'\n 'I received the same repetitive email from Lego!!'\n 'But succumbed for Xmas.'\n 'Still not received the parts and it is now the 8th Jan 2025.']\nRiassunto Gold: Customers are generally unhappy with their experiences with this company. \n\nCustomers mention encountering challenges with their orders. Customers are unhappy with the delivery service, finding it to be slow and unreliable. While the sentiment towards customer service is mixed, a number of customers express dissatisfaction with the communication and support provided. Customers also express dissatisfaction with the contact channels, finding them difficult to navigate and unresponsive. Additionally, there are negative sentiments regarding the refund process, with customers experiencing delays and difficulties. \n\nPunteggio RSA: ['‘What are the codes’…..'\n 'They had the AUDACITY to say we’ll send them for free!!'\n 'I’ve already paid over the odds for bits of plastic!!'\n 'I was greeted as I entered the store which was a pleasant start to the experience.'\n 'Never again.' 'Lego is already WAY OVERPRICED!!' 'Still not arrived.'\n 'I received the same repetitive email from Lego!!'\n 'But succumbed for Xmas.'\n 'Still not received the parts and it is now the 8th Jan 2025.']\n--------------------------------------------------------------------------------\nDocumento ID: ('UberEATS',)\nRiassunto Generato (best_rsa): ['I contacted uber support, and they told me to contact the restaurant, and now I haven&apos;t even received the order yet.'\n 'Extremely poor!'\n 'Uber Eats have just charged me over £27 for a cancelled order and I have not received my money back!'\n 'You can&apos;t talk to a person ever and no one knows - you literally can&apos;t even formally complain when the staff are incompetent and rude.'\n 'which I honestly think they didn&apos;t do.' 'Never have.'\n 'I will be using them again soon.' 'No chat, no phone, no email.'\n 'Please don&apos;t order from restaurants that deliver themself.' 'Wild.']\nRiassunto Gold: Customers express widespread dissatisfaction with this company. \n\nCustomers report negative experiences with the order process, frequently citing difficulties and complications.  While opinions are mixed on the delivery service, several customers mention experiencing delays and receiving cold food. Customers' feedback on the overall service is uncertain, indicating a lack of clarity regarding its quality.  Customers also express negativity towards the refund process, often describing it as frustrating and unsatisfactory. Lastly, customers widely agree that the customer service is subpar and unhelpful. \n\nPunteggio RSA: ['I contacted uber support, and they told me to contact the restaurant, and now I haven&apos;t even received the order yet.'\n 'Extremely poor!'\n 'Uber Eats have just charged me over £27 for a cancelled order and I have not received my money back!'\n 'You can&apos;t talk to a person ever and no one knows - you literally can&apos;t even formally complain when the staff are incompetent and rude.'\n 'which I honestly think they didn&apos;t do.' 'Never have.'\n 'I will be using them again soon.' 'No chat, no phone, no email.'\n 'Please don&apos;t order from restaurants that deliver themself.' 'Wild.']\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Evaluation - ROUGE","metadata":{}},{"cell_type":"code","source":"!pip install rouge-score\nfrom rouge_score import rouge_scorer\n\ndef evaluate_rouge(df):\n    # make a list of the tuples (text, summary)\n\n    texts = df.gold.tolist()\n    summaries = df.best_rsa.tolist()\n\n    # rouges\n    metrics = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": [], \"rougeLsum\": []}\n\n    rouges = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"], use_stemmer=True)\n    \n    metrics[\"rouge1\"].extend(\n        [\n            rouges.score(\"\".join(summary), text)[\"rouge1\"].fmeasure\n            for summary, text in zip(summaries, texts)\n        ]\n    )\n    metrics[\"rouge2\"].extend(\n        [\n            rouges.score(\"\".join(summary), text)[\"rouge2\"].fmeasure\n            for summary, text in zip(summaries, texts)\n        ]\n    )\n    metrics[\"rougeL\"].extend(\n        [\n            rouges.score(\"\".join(summary), text)[\"rougeL\"].fmeasure\n            for summary, text in zip(summaries, texts)\n        ]\n    )\n    metrics[\"rougeLsum\"].extend(\n        [\n            rouges.score(\"\".join(summary), text)[\"rougeLsum\"].fmeasure\n            for summary, text in zip(summaries, texts)\n        ]\n    )\n\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:24.606161Z","iopub.execute_input":"2025-02-14T13:22:24.606379Z","iopub.status.idle":"2025-02-14T13:22:30.697762Z","shell.execute_reply.started":"2025-02-14T13:22:24.606361Z","shell.execute_reply":"2025-02-14T13:22:30.696673Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge-score) (2024.2.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=b8b63240d46ebdec3d3f187c542fecad9b99e78f36c6315458ef49a2a884e749\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"df_results\ntexts = df_results.gold.tolist()\nsummaries = df_results.best_rsa.tolist()\nsummaries\nmetrics = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": [], \"rougeLsum\": []}\n\nrouges = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"], use_stemmer=True)\n    \nmetrics[\"rouge1\"].extend(\n    [\n        rouges.score(\"\".join(summary), text)[\"rouge1\"].fmeasure\n        for summary, text in zip(summaries, texts)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:30.698955Z","iopub.execute_input":"2025-02-14T13:22:30.699283Z","iopub.status.idle":"2025-02-14T13:22:30.939668Z","shell.execute_reply.started":"2025-02-14T13:22:30.699254Z","shell.execute_reply":"2025-02-14T13:22:30.939030Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"metrics = evaluate_rouge(df_results)\ndf_metrics = pd.DataFrame.from_dict(metrics)\nprint(df_metrics)\ndf_metrics.mean(axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:30.940697Z","iopub.execute_input":"2025-02-14T13:22:30.941045Z","iopub.status.idle":"2025-02-14T13:22:31.866413Z","shell.execute_reply.started":"2025-02-14T13:22:30.941014Z","shell.execute_reply":"2025-02-14T13:22:31.865652Z"}},"outputs":[{"name":"stdout","text":"     rouge1    rouge2    rougeL  rougeLsum\n0  0.269663  0.015094  0.127341   0.142322\n1  0.244275  0.015385  0.137405   0.167939\n2  0.341772  0.044586  0.145570   0.183544\n3  0.092141  0.000000  0.081301   0.070461\n4  0.243386  0.000000  0.105820   0.116402\n5  0.251163  0.037559  0.130233   0.176744\n6  0.176991  0.011869  0.088496   0.094395\n7  0.123563  0.005764  0.094828   0.091954\n8  0.188679  0.000000  0.113208   0.125786\n9  0.127660  0.010753  0.095745   0.095745\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"rouge1       0.205929\nrouge2       0.014101\nrougeL       0.111994\nrougeLsum    0.126529\ndtype: float64"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"## BERTSCORE","metadata":{}},{"cell_type":"code","source":"! pip install bert-score\nfrom bert_score import BERTScorer\ndef evaluate_bartbert(df, device=\"cuda\"):\n    # make a list of the tuples (text, summary)\n\n    texts = df.gold.tolist()\n    summaries = df.best_rsa.tolist()\n\n    scorer = BERTScorer(lang=\"en\", rescale_with_baseline=False, device=device)\n\n    metrics = {'BERTScore': []}\n    for i in range(len(texts)):\n        texts[i] = texts[i].replace(\"\\n\", \" \")\n        summ= \"\".join(summaries[i])\n        summ = summ.replace(\"\\n\", \" \")\n\n        P, R, F1 = scorer.score([summ], [texts[i]])\n\n        metrics['BERTScore'].append(F1.mean().item())\n\n    # compute the mean of the metrics\n    # metrics = {k: sum(v) / len(v) for k, v in metrics.items()}\n\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:31.867252Z","iopub.execute_input":"2025-02-14T13:22:31.867472Z","iopub.status.idle":"2025-02-14T13:22:35.639222Z","shell.execute_reply.started":"2025-02-14T13:22:31.867454Z","shell.execute_reply":"2025-02-14T13:22:35.638353Z"}},"outputs":[{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.5.1+cu121)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.2.2)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.47.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert-score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert-score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.27.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.12.14)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert-score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert-score\nSuccessfully installed bert-score-0.3.13\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"metrics = evaluate_bartbert(df_results)\n# make a dataframe with the metric\ndf_bert_metrics = pd.DataFrame(metrics)\nprint(df_bert_metrics)\ndf_bert_metrics.mean(axis=0)\n#this happens because we are comparing two complete paragraphs. We should do it on a sentence level and maybe get a similarity matrix and see if each sentence find a twin.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:35.640383Z","iopub.execute_input":"2025-02-14T13:22:35.640706Z","iopub.status.idle":"2025-02-14T13:22:45.721191Z","shell.execute_reply.started":"2025-02-14T13:22:35.640677Z","shell.execute_reply":"2025-02-14T13:22:45.720119Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42aa00ef7fcf4fc881ef6853828a2939"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84edc34521024e0aa8b14510de2c3721"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d38e482872f4449d82541d229ddc306f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35f3eff67db24ab896d374de286449a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ca08d3dcba44a18fe18883465343a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91e102024a354bf39115ca7e8211d679"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"   BERTScore\n0   0.822756\n1   0.829242\n2   0.849976\n3   0.800878\n4   0.834533\n5   0.832915\n6   0.816143\n7   0.765681\n8   0.817703\n9   0.819433\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"BERTScore    0.818926\ndtype: float64"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\n\ndef evaluate_bertscore(df, device=\"cuda\"):\n    # make a list of the tuples (text, summary)\n\n    texts = df.gold.tolist()\n    summaries = df.best_rsa.tolist()\n    scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True, device=device)\n    bert_scores = []\n    for text,summary in zip(texts,summaries):\n        text_sentences = sent_tokenize(text)\n        summary_sentences = summary\n        scores = np.zeros((len(text_sentences), len(summary_sentences)))\n        for i, sent1 in enumerate(text_sentences):\n            for j, sent2 in enumerate(summary_sentences):\n                P, R, F1 = scorer.score([sent1], [sent2])\n                scores[i, j] = P.item()\n        precision= np.max(scores, axis=1).mean()\n        bert_scores.append(precision)\n    return np.array(bert_scores)\nmetrics = evaluate_bertscore(df_results)\n# make a dataframe with the metric\ndf_bert_metrics = pd.DataFrame(metrics)\nprint(df_bert_metrics)\ndf_bert_metrics.mean(axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:45.722164Z","iopub.execute_input":"2025-02-14T13:22:45.722489Z","iopub.status.idle":"2025-02-14T13:22:58.813161Z","shell.execute_reply.started":"2025-02-14T13:22:45.722454Z","shell.execute_reply":"2025-02-14T13:22:58.812251Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"          0\n0  0.254969\n1  0.216895\n2  0.359022\n3  0.291223\n4  0.231544\n5  0.264558\n6  0.228156\n7  0.317096\n8  0.302271\n9  0.283428\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0    0.274916\ndtype: float64"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## SEAHORSE","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nmap_questionnumber_to_question = {\n    \"question1\": \"SHMetric/Comprehensible\",\n    \"question2\": \"SHMetric/Repetition\",\n    \"question3\": \"SHMetric/Grammar\",\n    \"question4\": \"SHMetric/Attribution\",\n    \"question5\": \"SHMetric/Main ideas\",\n    \"question6\": \"SHMetric/Conciseness\",\n}\ndef evaluate_classification_task(model, tokenizer, question, df, batch_size):\n\n    texts = df.gold.tolist()\n    summaries = df.best_rsa.tolist()\n    template = \"premise: {premise} hypothesis: {hypothesis}\"\n    ds = [template.format(premise=text[:20*1024], hypothesis=\"\".join(summary)) for text, summary in zip(texts, summaries)]\n\n\n    eval_loader = torch.utils.data.DataLoader(ds, batch_size=batch_size)\n\n    metrics = {f\"{question}/proba_1\": [], f\"{question}/proba_0\": [], f\"{question}/guess\": []}\n\n    with torch.no_grad():\n        for batch in tqdm(eval_loader):\n            # tokenize the batch\n            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n            # move the inputs to the device\n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n            N_inputs = inputs[\"input_ids\"].shape[0]\n            # make decoder inputs to be <pad>\n            decoder_input_ids = torch.full((N_inputs, 1), tokenizer.pad_token_id, dtype=torch.long, device=model.device)\n\n            outputs = model(**inputs, decoder_input_ids=decoder_input_ids)\n            logits = outputs.logits\n            # retrieve logits for the last token and the scores for 0 and 1\n            logits = logits[:, -1, [497, 333]]\n\n            # compute the probabilities\n            probs = F.softmax(logits, dim=-1)\n\n            # compute the guess\n            guess = probs.argmax(dim=-1)\n\n            # append the metrics\n            metrics[f\"{question}/proba_1\"].extend(probs[:, 1].tolist())\n            metrics[f\"{question}/proba_0\"].extend(probs[:, 0].tolist())\n            metrics[f\"{question}/guess\"].extend(guess.tolist())\n\n    # average the metrics\n\n    # metrics = {k: sum(v) / len(v) for k, v in metrics.items()}\n\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:58.814101Z","iopub.execute_input":"2025-02-14T13:22:58.814419Z","iopub.status.idle":"2025-02-14T13:22:58.822571Z","shell.execute_reply.started":"2025-02-14T13:22:58.814387Z","shell.execute_reply":"2025-02-14T13:22:58.821648Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"metric_seahorse = {}\nfor index,question in enumerate(map_questionnumber_to_question.values()):\n    model_name = f\"google/seahorse-large-q{index+1}\"\n    print(question)   \n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\n    metrics = evaluate_classification_task(model, tokenizer, question, df_results, 16)\n    metric_seahorse[question] = metrics[f\"{question}/guess\"]\nmetric_seahorse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:22:58.823450Z","iopub.execute_input":"2025-02-14T13:22:58.823736Z","iopub.status.idle":"2025-02-14T13:27:05.978845Z","shell.execute_reply.started":"2025-02-14T13:22:58.823712Z","shell.execute_reply":"2025-02-14T13:27:05.978023Z"}},"outputs":[{"name":"stdout","text":"SHMetric/Comprehensible\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"967cb6fd6b644a4f84a008df2f48783c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20112deefef4ce59cdc40a007303afb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42dc55a6870429bba215b5ddd3270fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19824271070647f1a60ad137714a81c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3560456a280f4657bc9e824fd2e3a1b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7bce134f70b4d7f8d1783622e4a52b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6c83fe1756c434f88d8f2b0b289dfaa"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"SHMetric/Repetition\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e4f9ec53fbb47aaab1653b2074c17a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55328c902fe74863985f4c79d02dfb34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122d0008d81c4a54ba14c6b291a6ffd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48912373e8e843db88f58185a81f6cd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e276ec1ffe64515bcfc9c4b5713aebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37facc617f784f4ba9e897f6116a0965"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6812dba51974e94bc385322f1ee8725"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n100%|██████████| 1/1 [00:02<00:00,  2.03s/it]","output_type":"stream"},{"name":"stdout","text":"SHMetric/Grammar\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6879a2f501004a9eb14e5c5af803d603"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5731fdf494f74317a503abe42678a535"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9fca355f28c4a72a2f740bb7a8c9ea1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"722d6d8d91b247cbb0609dba9af07e91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08e2e892e339407a821bd7fb6035a4a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f66ff5e471a44abb9c735a3df945e61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ea7a96eb6049ea86c9a306cef278d3"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"SHMetric/Attribution\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"968b14f45b2344fd9a31feb867e19172"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bf9fdaa8cc74cd4bd17585a7df1b5fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c23bd30b534b718d54256e4c0ba48d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e473d43aaf3400aa9e906f84f7a0f7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9cac92e1138413ba3201865bacf7dd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6e082d4b8b7496996a70524c522fa5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b95452cc9f141cc85775fda4dfbae2c"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"SHMetric/Main ideas\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21a1ad7eee5e446589143852125ae375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5fac935a6564e8e970a58b1ff62dee2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f8da0af216743be87637c852d4cf081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc3edb145544eb397ef83f0245c0f39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b965018f70404ef69a0669e9cd698f7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203c4845bbcd402ca6a5bded0f06023b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0177b506470140c6a9523b58b044067e"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"SHMetric/Conciseness\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e320ffbedfe343d8bd2b723c950fa4ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"644220cb575f4b6985e833c56159957e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"056fb0e7440c496b80e68ccbad395d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9253abbdff4a379a5bc199404bcd33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edfa6769194b420cb208275ac63e7dd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c6aea20d5ae42c3b5f467cffb69fd6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb242ffb88184690b89b53f29e84dce0"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'SHMetric/Comprehensible': [1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n 'SHMetric/Repetition': [1, 1, 1, 0, 1, 1, 0, 0, 1, 1],\n 'SHMetric/Grammar': [0, 1, 1, 0, 1, 0, 0, 0, 1, 1],\n 'SHMetric/Attribution': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'SHMetric/Main ideas': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'SHMetric/Conciseness': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"df_seahorse_metrics = pd.DataFrame(metric_seahorse)\ndf_seahorse_metrics\ndf_seahorse_metrics.mean(axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:27:05.979928Z","iopub.execute_input":"2025-02-14T13:27:05.980300Z","iopub.status.idle":"2025-02-14T13:27:05.990103Z","shell.execute_reply.started":"2025-02-14T13:27:05.980253Z","shell.execute_reply":"2025-02-14T13:27:05.989280Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"SHMetric/Comprehensible    0.9\nSHMetric/Repetition        0.7\nSHMetric/Grammar           0.5\nSHMetric/Attribution       0.0\nSHMetric/Main ideas        0.0\nSHMetric/Conciseness       0.0\ndtype: float64"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## TEST ROBERTA to detect linguistically acceptable sentences","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"textattack/roberta-base-CoLA\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"textattack/roberta-base-CoLA\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:27:05.990985Z","iopub.execute_input":"2025-02-14T13:27:05.991184Z","iopub.status.idle":"2025-02-14T13:27:12.173223Z","shell.execute_reply.started":"2025-02-14T13:27:05.991167Z","shell.execute_reply":"2025-02-14T13:27:12.172257Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d24396ca644a6b88789418b8028f47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3596e312354001ab818b78ead57e08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2779a8a743041dca5b0528b37332874"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d064ae288c374309849b9a9b4c647be2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c0d9a7e5c0e4316a6ac85def578caef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eed2836a659d42cbab5c59b277f11015"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaForCausalLM, RobertaTokenizer\nimport torch\nimport re\ntokenizer = AutoTokenizer.from_pretrained(\"textattack/roberta-base-CoLA\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"textattack/roberta-base-CoLA\")\n\ndef check_if_sentence(text):\n    \n    text = text.strip()\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n    output = model(**inputs)\n    model_output = bool(output.logits.argmax(dim=-1).item())\n    print(output.logits.argmax(dim=-1).item())\n    sentence_start = text[0].isupper()\n    sentence_end = bool(re.search(r'[.!?]$', text))\n\n    if (model_output):\n        return True\n    else:\n        return False\nsentence = \"Here are some comments\"\ncheck_if_sentence(sentence) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:27:12.174885Z","iopub.execute_input":"2025-02-14T13:27:12.175179Z","iopub.status.idle":"2025-02-14T13:27:13.550398Z","shell.execute_reply.started":"2025-02-14T13:27:12.175153Z","shell.execute_reply":"2025-02-14T13:27:13.549449Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"1\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom huggingface_hub import PyTorchModelHubMixin\n\n\n\nclass QualityModel(nn.Module, PyTorchModelHubMixin):\n    def __init__(self, config):\n        super(QualityModel, self).__init__()\n        self.model = AutoModel.from_pretrained(config[\"base_model\"])\n        self.dropout = nn.Dropout(config[\"fc_dropout\"])\n        self.fc = nn.Linear(self.model.config.hidden_size, len(config[\"id2label\"]))\n\n    def forward(self, input_ids, attention_mask):\n        features = self.model(\n            input_ids=input_ids, attention_mask=attention_mask\n        ).last_hidden_state\n        dropped = self.dropout(features)\n        outputs = self.fc(dropped)\n        return torch.softmax(outputs[:, 0, :], dim=1)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nconfig = AutoConfig.from_pretrained(\"nvidia/quality-classifier-deberta\")\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/quality-classifier-deberta\")\nmodel = QualityModel.from_pretrained(\"nvidia/quality-classifier-deberta\").to(device)\nmodel.eval()\n\n\n# Prepare and process inputs\ntext_samples = [\"I think that the paper is very well written, I like it\",\"The authors localized a phenomenon and demonstrated how to exploit it.\",\"I trust the results because I performed exactly the same experiments for CIFAR-10 with longer non-regularization periods and found that there is no effect (this is also that the authors show in the paper)  but I didn't test on other datasets and obviously didn't think about potential benefits for compression.\",\".?@fdsa Low quality text.\", \"I like pizza because it provides several nutritients while being tasty\",\"To tell means express something in words.\",\"We concluded in the previous section that the classifiers output a different kind of informativeness than the human annotations.\",\"Here are some comments\",\"Still, extractive summarization methods are notably sensitive to the sentence segmentation process\",\" which can occasionally result in peculiar outcomes\"]\ninputs = tokenizer(\n    text_samples, return_tensors=\"pt\", padding=\"longest\", truncation=True\n).to(device)\noutputs = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n\n# Predict and display results\npredicted_classes = torch.argmax(outputs, dim=1)\npredicted_domains = [\n    config.id2label[class_idx.item()] for class_idx in predicted_classes.cpu().numpy()\n]\nprint(predicted_domains)\nprint(predicted_classes)\nprint(outputs)\nquality_scores_raw = outputs.cpu().detach().numpy()\n#to get a quality score, we can sum the probability of \"High\" and 0.7 times the probability of \"Medium\". This is a simple heuristic to get a score between 0 and 1. \"Medium\" is multiplied by 0.7 to give it less importance than \"High\".\nquality_scores = quality_scores_raw[:,0]+ 0.7*quality_scores_raw[:,1]\nquality_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:27:13.551689Z","iopub.execute_input":"2025-02-14T13:27:13.551983Z","iopub.status.idle":"2025-02-14T13:27:37.846508Z","shell.execute_reply.started":"2025-02-14T13:27:13.551960Z","shell.execute_reply":"2025-02-14T13:27:37.845494Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c999cb3cdfc43709f71a4a5b1e220c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c57971d28304c7e8b37e806971b56b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15262e5262d54e9fb9bea511c3161b3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/8.66M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"821cf4cc4eef4e7f957d8a2957ab503a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f0c9815651c4b31a6734f36a787bac4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/286 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bff6630e5e64f86b7db9b8e144c40da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb37ca8f617d4e9db82f79356c717adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e51ed09ba35412c986f47039757e90c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/735M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb67e0dd5394b1f919c1b0d3a654df3"}},"metadata":{}},{"name":"stdout","text":"['Medium', 'High', 'Medium', 'Low', 'Medium', 'Medium', 'High', 'Medium', 'Medium', 'Medium']\ntensor([1, 0, 1, 2, 1, 1, 0, 1, 1, 1], device='cuda:0')\ntensor([[6.7445e-03, 8.8790e-01, 1.0536e-01],\n        [4.9476e-01, 4.5453e-01, 5.0715e-02],\n        [3.4836e-02, 9.5611e-01, 9.0515e-03],\n        [5.8394e-04, 2.6585e-02, 9.7283e-01],\n        [2.8301e-02, 9.4528e-01, 2.6421e-02],\n        [1.6381e-01, 7.4715e-01, 8.9036e-02],\n        [6.1041e-01, 3.5894e-01, 3.0653e-02],\n        [4.0138e-03, 8.0881e-01, 1.8718e-01],\n        [3.4625e-01, 6.0633e-01, 4.7417e-02],\n        [1.7532e-01, 7.5197e-01, 7.2714e-02]], device='cuda:0',\n       grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"array([0.6282743 , 0.812927  , 0.7041147 , 0.01919315, 0.6899959 ,\n       0.68681777, 0.8616661 , 0.57018   , 0.7706837 , 0.7016957 ],\n      dtype=float32)"},"metadata":{}}],"execution_count":26}]}